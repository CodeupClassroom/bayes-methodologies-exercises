{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP: Classify Articles from Titles\n",
    "\n",
    "We will walk through an end-to-end example of taking the raw text of the articles' titles, clean it, tokenize and vectorize it, explore it, and develop a model to classify each article into one of 4 labeled classes (business, sports, technology, or entertainment). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# matplotlib default plotting styles\n",
    "plt.rc(\"patch\", edgecolor=\"black\", force_edgecolor=True)\n",
    "plt.rc(\"axes\", grid=True)\n",
    "plt.rc(\"grid\", linestyle=\":\", linewidth=0.8, alpha=0.7)\n",
    "plt.rc(\"axes.spines\", right=False, top=False)\n",
    "plt.rc(\"figure\", figsize=(11, 8))\n",
    "plt.rc(\"font\", size=12.0)\n",
    "plt.rc(\"hist\", bins=25)\n",
    "\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire: Scrape InShorts.com\n",
    "\n",
    "Acquire news articles from [InShorts](inshorts.com) using `BeautifulSoup` and `get.requests`\n",
    "\n",
    "- [business](https://inshorts.com/en/read/business)\n",
    "- [sports](https://inshorts.com/en/read/sports)\n",
    "- [technology](https://inshorts.com/en/read/technology)\n",
    "- [entertainment](https://inshorts.com/en/read/entertainment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = acquire.get_news_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>category</th>\n",
       "      <th>published_date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Dharna</td>\n",
       "      <td>The Delhi Police has seized over 2,000 fake Xi...</td>\n",
       "      <td>business</td>\n",
       "      <td>2019-12-07T06:30:22.000Z</td>\n",
       "      <td>Fake Xiaomi products worth ₹13 lakh seized fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Krishna Veera Vanamali</td>\n",
       "      <td>After a US jury found that Elon Musk did not d...</td>\n",
       "      <td>business</td>\n",
       "      <td>2019-12-07T16:34:59.000Z</td>\n",
       "      <td>My faith in humanity is restored: Musk after w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Krishna Veera Vanamali</td>\n",
       "      <td>British cave explorer Vernon Unsworth, who los...</td>\n",
       "      <td>business</td>\n",
       "      <td>2019-12-07T15:31:16.000Z</td>\n",
       "      <td>I'll take it on the chin: Cave explorer after ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Krishna Veera Vanamali</td>\n",
       "      <td>SoftBank CEO Masayoshi Son has said the decisi...</td>\n",
       "      <td>business</td>\n",
       "      <td>2019-12-06T16:27:31.000Z</td>\n",
       "      <td>We are the same animal, we are both a little c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Krishna Veera Vanamali</td>\n",
       "      <td>Airtel has fixed a security flaw that could ha...</td>\n",
       "      <td>business</td>\n",
       "      <td>2019-12-07T09:45:04.000Z</td>\n",
       "      <td>23-yr-old finds Airtel app bug that could have...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  author  \\\n",
       "0           0                  Dharna   \n",
       "1           1  Krishna Veera Vanamali   \n",
       "2           2  Krishna Veera Vanamali   \n",
       "3           3  Krishna Veera Vanamali   \n",
       "4           4  Krishna Veera Vanamali   \n",
       "\n",
       "                                                body  category  \\\n",
       "0  The Delhi Police has seized over 2,000 fake Xi...  business   \n",
       "1  After a US jury found that Elon Musk did not d...  business   \n",
       "2  British cave explorer Vernon Unsworth, who los...  business   \n",
       "3  SoftBank CEO Masayoshi Son has said the decisi...  business   \n",
       "4  Airtel has fixed a security flaw that could ha...  business   \n",
       "\n",
       "             published_date                                              title  \n",
       "0  2019-12-07T06:30:22.000Z  Fake Xiaomi products worth ₹13 lakh seized fro...  \n",
       "1  2019-12-07T16:34:59.000Z  My faith in humanity is restored: Musk after w...  \n",
       "2  2019-12-07T15:31:16.000Z  I'll take it on the chin: Cave explorer after ...  \n",
       "3  2019-12-06T16:27:31.000Z  We are the same animal, we are both a little c...  \n",
       "4  2019-12-07T09:45:04.000Z  23-yr-old finds Airtel app bug that could have...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only going to work with the article titles in this example. \n",
    "We will select title and category (our target variable) and assign those to a new dataframe that we will work with moving forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = original_df[['title','category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fake Xiaomi products worth ₹13 lakh seized fro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My faith in humanity is restored: Musk after w...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'll take it on the chin: Cave explorer after ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We are the same animal, we are both a little c...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23-yr-old finds Airtel app bug that could have...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  category\n",
       "0  Fake Xiaomi products worth ₹13 lakh seized fro...  business\n",
       "1  My faith in humanity is restored: Musk after w...  business\n",
       "2  I'll take it on the chin: Cave explorer after ...  business\n",
       "3  We are the same animal, we are both a little c...  business\n",
       "4  23-yr-old finds Airtel app bug that could have...  business"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare\n",
    "### Normalize Text\n",
    "\n",
    "- Convert to all lowercase  \n",
    "- Normalize the unicode chars  \n",
    "- Remove any non-alpha or whitespace characters  \n",
    "- Remove any alpha strings with 2 characters or less  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake Xiaomi products worth ₹13 lakh seized from Delhi's Gaffar Market\n",
      "fake xiaomi products worth ₹13 lakh seized from delhi's gaffar market\n",
      "fake xiaomi products worth 13 lakh seized from delhi's gaffar market\n",
      "fake xiaomi products worth    lakh seized from delhi s gaffar market\n",
      "fake xiaomi products worth    lakh seized from delhi  gaffar market\n",
      "fake xiaomi products worth    lakh seized from delhi  gaffar market\n",
      "fake xiaomi products worth lakh seized from delhi gaffar market\n",
      "fake xiaomi products worth lakh seized from delhi gaffar market\n"
     ]
    }
   ],
   "source": [
    "string = df.title[0]\n",
    "print(string)\n",
    "string = string.lower()\n",
    "print(string)\n",
    "string = unicodedata.normalize('NFKD', string).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "print(string)\n",
    "\n",
    "string = re.sub(r'[^a-z]', ' ', string)\n",
    "print(string)\n",
    "\n",
    "string = re.sub(r'\\b[a-z]{,2}\\b', '', string)\n",
    "print(string)\n",
    "\n",
    "string = re.sub(r'[\\r|\\n|\\r\\n]+', ' ', string)\n",
    "print(string)\n",
    "\n",
    "string = re.sub(r'\\s+', ' ', string)\n",
    "print(string)\n",
    "\n",
    "string = string.strip()\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(string):\n",
    "    \"\"\"\n",
    "    Convert to all lowercase  \n",
    "    Normalize the unicode chars  \n",
    "    Remove any non-alpha or whitespace characters  \n",
    "    Remove any alpha strings with 2 characters or less  \n",
    "    \"\"\"\n",
    "    string = string.lower()\n",
    "    string = unicodedata.normalize('NFKD', string).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "    # keep only alpha chars\n",
    "    string = re.sub(r'[^a-z]', ' ', string)\n",
    "    \n",
    "    # remove strings less than 2 chars in length\n",
    "    string = re.sub(r'\\b[a-z]{,2}\\b', '', string)\n",
    "    \n",
    "    # convert newlines and tabs to a single space\n",
    "    string = re.sub(r'[\\r|\\n|\\r\\n]+', ' ', string)\n",
    "    \n",
    "    # strip extra whitespace\n",
    "    string = string.strip()\n",
    "    \n",
    "    return string\n",
    "\n",
    "df = df.assign(normalized = df.title.apply(normalize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake xiaomi products worth lakh seized from delhi gaffar market\n"
     ]
    }
   ],
   "source": [
    "print(string)\n",
    "\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "stems = [ps.stem(word) for word in string.split()]\n",
    "stem_string = ' '.join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    string_of_stems = ' '.join(stems)\n",
    "    return string_of_stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake xiaomi products worth lakh seized from delhi gaffar market\n",
      "fake xiaomi product worth lakh seized from delhi gaffar market\n",
      "fake xiaomi product worth lakh seiz from delhi gaffar market\n"
     ]
    }
   ],
   "source": [
    "print(string)\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "lemma_string = ' '.join(lemmas)\n",
    "print(lemma_string)\n",
    "print(stem_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    string_of_lemmas = ' '.join(lemmas)\n",
    "    return string_of_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake xiaomi products worth lakh seized from delhi gaffar market\n"
     ]
    }
   ],
   "source": [
    "print(string)\n",
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "tokenized_string = tokenizer.tokenize(string, return_str=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    return tokenizer.tokenize(string, return_str=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fake', 'xiaomi', 'products', 'worth', 'lakh', 'seized', 'from', 'delhi', 'gaffar', 'market']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fake xiaomi products worth lakh seized delhi gaffar market'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = tokenized_string.split()\n",
    "print(words)\n",
    "stopwords_list = stopwords.words('english')\n",
    "exclude_words = ['me']\n",
    "stopwords_list = set(stopwords_list) - set(exclude_words)\n",
    "include_words = ['Dom']\n",
    "stopwords_list = stopwords_list.union(set(include_words))\n",
    "filtered_words = [w for w in words if w not in stopwords_list]\n",
    "filtered_words = ' '.join(filtered_words)\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_string, extra_words=[], exclude_words=[]):\n",
    "    words = tokenized_string.split()\n",
    "    stopword_list = stopwords.words('english')\n",
    "\n",
    "    # remove the excluded words from the stopword list\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "\n",
    "    # add in the user specified extra words\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "\n",
    "    filtered_words = [w for w in words if w not in stopword_list]\n",
    "    final_string = \" \".join(filtered_words)\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fake Xiaomi products worth ₹13 lakh seized fro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My faith in humanity is restored: Musk after w...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'll take it on the chin: Cave explorer after ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We are the same animal, we are both a little c...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23-yr-old finds Airtel app bug that could have...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  category\n",
       "0  Fake Xiaomi products worth ₹13 lakh seized fro...  business\n",
       "1  My faith in humanity is restored: Musk after w...  business\n",
       "2  I'll take it on the chin: Cave explorer after ...  business\n",
       "3  We are the same animal, we are both a little c...  business\n",
       "4  23-yr-old finds Airtel app bug that could have...  business"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = original_df[['title','category']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>original</th>\n",
       "      <th>normalized</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fake Xiaomi products worth ₹13 lakh seized fro...</td>\n",
       "      <td>business</td>\n",
       "      <td>Fake Xiaomi products worth ₹13 lakh seized fro...</td>\n",
       "      <td>fake xiaomi products worth    lakh seized from...</td>\n",
       "      <td>fake xiaomi product worth lakh seiz from delhi...</td>\n",
       "      <td>fake xiaomi product worth lakh seized from del...</td>\n",
       "      <td>fake xiaomi product worth lakh seiz delhi gaff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My faith in humanity is restored: Musk after w...</td>\n",
       "      <td>business</td>\n",
       "      <td>My faith in humanity is restored: Musk after w...</td>\n",
       "      <td>faith  humanity  restored  musk after winning ...</td>\n",
       "      <td>faith human restor musk after win defam case</td>\n",
       "      <td>faith humanity restored musk after winning def...</td>\n",
       "      <td>faith human restor musk win defam case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'll take it on the chin: Cave explorer after ...</td>\n",
       "      <td>business</td>\n",
       "      <td>I'll take it on the chin: Cave explorer after ...</td>\n",
       "      <td>take   the chin  cave explorer after losing ca...</td>\n",
       "      <td>take the chin cave explor after lose case agai...</td>\n",
       "      <td>take the chin cave explorer after losing case ...</td>\n",
       "      <td>take chin cave explor lose case elon musk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We are the same animal, we are both a little c...</td>\n",
       "      <td>business</td>\n",
       "      <td>We are the same animal, we are both a little c...</td>\n",
       "      <td>are the same animal   are both  little crazy  ...</td>\n",
       "      <td>are the same anim are both littl crazi masayos...</td>\n",
       "      <td>are the same animal are both little crazy masa...</td>\n",
       "      <td>anim littl crazi masayoshi son jack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23-yr-old finds Airtel app bug that could have...</td>\n",
       "      <td>business</td>\n",
       "      <td>23-yr-old finds Airtel app bug that could have...</td>\n",
       "      <td>old finds airtel app bug that could have expos...</td>\n",
       "      <td>old find airtel app bug that could have expos ...</td>\n",
       "      <td>old find airtel app bug that could have expose...</td>\n",
       "      <td>old find airtel app bug could expos data crore...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  category  \\\n",
       "0  Fake Xiaomi products worth ₹13 lakh seized fro...  business   \n",
       "1  My faith in humanity is restored: Musk after w...  business   \n",
       "2  I'll take it on the chin: Cave explorer after ...  business   \n",
       "3  We are the same animal, we are both a little c...  business   \n",
       "4  23-yr-old finds Airtel app bug that could have...  business   \n",
       "\n",
       "                                            original  \\\n",
       "0  Fake Xiaomi products worth ₹13 lakh seized fro...   \n",
       "1  My faith in humanity is restored: Musk after w...   \n",
       "2  I'll take it on the chin: Cave explorer after ...   \n",
       "3  We are the same animal, we are both a little c...   \n",
       "4  23-yr-old finds Airtel app bug that could have...   \n",
       "\n",
       "                                          normalized  \\\n",
       "0  fake xiaomi products worth    lakh seized from...   \n",
       "1  faith  humanity  restored  musk after winning ...   \n",
       "2  take   the chin  cave explorer after losing ca...   \n",
       "3  are the same animal   are both  little crazy  ...   \n",
       "4  old finds airtel app bug that could have expos...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  fake xiaomi product worth lakh seiz from delhi...   \n",
       "1       faith human restor musk after win defam case   \n",
       "2  take the chin cave explor after lose case agai...   \n",
       "3  are the same anim are both littl crazi masayos...   \n",
       "4  old find airtel app bug that could have expos ...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  fake xiaomi product worth lakh seized from del...   \n",
       "1  faith humanity restored musk after winning def...   \n",
       "2  take the chin cave explorer after losing case ...   \n",
       "3  are the same animal are both little crazy masa...   \n",
       "4  old find airtel app bug that could have expose...   \n",
       "\n",
       "                                             cleaned  \n",
       "0  fake xiaomi product worth lakh seiz delhi gaff...  \n",
       "1             faith human restor musk win defam case  \n",
       "2          take chin cave explor lose case elon musk  \n",
       "3                anim littl crazi masayoshi son jack  \n",
       "4  old find airtel app bug could expos data crore...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = original_df[['title','category']]\n",
    "df = df.assign(original = df.title)\n",
    "\n",
    "df = df.assign(normalized = df.original.apply(normalize))\n",
    "\n",
    "df = df.assign(stemmed = df.normalized.apply(stem))\n",
    "df = df.assign(lemmatized = df.normalized.apply(lemmatize))\n",
    "\n",
    "df = df.assign(cleaned = df.stemmed.apply(remove_stopwords))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_articles(df):\n",
    "    df = df.assign(original = df.title)\n",
    "    df = df.assign(normalized = df.original.apply(normalize))\n",
    "    df = df.assign(stemmed = df.normalized.apply(stem))\n",
    "    df = df.assign(lemmatized = df.normalized.apply(lemmatize))\n",
    "    df = df.assign(cleaned = df.stemmed.apply(remove_stopwords))\n",
    "    df.drop(columns=[\"title\"], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangle Summary (Acquire + Prep)\n",
    "\n",
    "1. acquire data via web scraping  \n",
    "2. select the column of text to be used as features and the target variable  \n",
    "3. prepare the text  \n",
    "    - normalize the text  \n",
    "    - lemmatize the normalized text  \n",
    "    - remove stopwords from the lemmatized text  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_news_articles(text, target):\n",
    "    df = acquire.get_news_articles()\n",
    "    df = df[[text, target]]\n",
    "    df = prep_articles(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prep_news_articles('title','category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>fake xiaomi product worth lakh seiz delhi gaff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>faith human restor musk win defam case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>take chin cave explor lose case elon musk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>anim littl crazi masayoshi son jack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>old find airtel app bug could expos data crore...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                            cleaned\n",
       "0  business  fake xiaomi product worth lakh seiz delhi gaff...\n",
       "1  business             faith human restor musk win defam case\n",
       "2  business          take chin cave explor lose case elon musk\n",
       "3  business                anim littl crazi masayoshi son jack\n",
       "4  business  old find airtel app bug could expos data crore..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['category','cleaned']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore & Define Features\n",
    "\n",
    "We will do a very quick exploration, due to time, which will naturally blend into feature engineering.  \n",
    "\n",
    "### Category Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sports           0.252525\n",
       "technology       0.252525\n",
       "entertainment    0.252525\n",
       "business         0.242424\n",
       "Name: category, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sports</th>\n",
       "      <td>25</td>\n",
       "      <td>0.252525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>technology</th>\n",
       "      <td>25</td>\n",
       "      <td>0.252525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>25</td>\n",
       "      <td>0.252525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>24</td>\n",
       "      <td>0.242424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                n   percent\n",
       "sports         25  0.252525\n",
       "technology     25  0.252525\n",
       "entertainment  25  0.252525\n",
       "business       24  0.242424"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(\n",
    "    [df.category.value_counts(), df.category.value_counts(normalize=True)], axis=1\n",
    ").set_axis([\"n\", \"percent\"], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency\n",
    "\n",
    "Generally, when you hear 'Term Frequency', it is referring to the number of documents in which a word appears.  \n",
    "*When we move on to computing TF-IDF, this basic definition of Term Frequency applies*.  \n",
    "\n",
    "However, term frequency can be calculated in a number of ways, all of which reflect how frequently a word appears in a document.  \n",
    "\n",
    "- **Raw Count**: This is simply the count of the number of occurances of each word.\n",
    "- **Frequency**: The number of times each word appears divided by the total number of words.\n",
    "- **Augmented Frequency**: The frequency of each word divided by the maximum frequency. This can help prevent bias towards larger documents.\n",
    "\n",
    "One way I will use term frequency here is to remove any term from the documents if that term frequency is 1. There is no value and will likely lead to overfitting when the term only appears in 1 article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_count = pd.Series(\" \".join(df.cleaned).split()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '10 most common words')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_count.sort_values(ascending=False)[0:10].plot.barh(width=.9)\n",
    "plt.title('10 most common words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['india', 'report', 'lose', 'case', 'old', 'musk', 'take', 'singh',\n",
       "       'user', 'film',\n",
       "       ...\n",
       "       'khan', 'ayushmann', 'import', 'season', 'nba', 'athlet', 'speak',\n",
       "       'kabir', 'year', 'yuvraj'],\n",
       "      dtype='object', length=574)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_count.index"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_count</th>\n",
       "      <th>tf</th>\n",
       "      <th>augmented_frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>india</th>\n",
       "      <td>12</td>\n",
       "      <td>0.036474</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>report</th>\n",
       "      <td>7</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lose</th>\n",
       "      <td>5</td>\n",
       "      <td>0.015198</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case</th>\n",
       "      <td>5</td>\n",
       "      <td>0.015198</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>old</th>\n",
       "      <td>5</td>\n",
       "      <td>0.015198</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>musk</th>\n",
       "      <td>5</td>\n",
       "      <td>0.015198</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        raw_count        tf  augmented_frequency\n",
       "word                                            \n",
       "india          12  0.036474             1.000000\n",
       "report          7  0.021277             0.583333\n",
       "lose            5  0.015198             0.416667\n",
       "case            5  0.015198             0.416667\n",
       "old             5  0.015198             0.416667\n",
       "musk            5  0.015198             0.416667"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_df = pd.DataFrame({'word': raw_count.index, 'raw_count': raw_count}).set_index('word')\n",
    "tf_df = tf_df[tf_df.raw_count > 1]\n",
    "tf_df = tf_df.assign(tf=lambda df: df.raw_count / df.raw_count.sum())\\\n",
    "    .assign(augmented_frequency=lambda df: df.tf / df.tf.max())\n",
    "\n",
    "tf_df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)\n",
    "\n",
    "Inverse Document Frequency also provides information about individual words, but, in order to use this measure, we must have multiple documents, i.e. several different bodies of text.\n",
    "\n",
    "- provides information about individual words in a document with respect the those words' frequency across the corpus, which must include multiple documents.  \n",
    "- indicates how valuable a word is in terms of the information it provides.\n",
    "- The more frequently a word appears in the corpus, the less information it provides when found in a certain document. This will lead to a lower IDF score. \n",
    "- Lower IDF $\\rightarrow$ less important when found in a document, i.e. that word gives us less information about the document than a word with a high IDF. \n",
    "- Example: 'the' would have a *very* low IDF which indicates we learn very little about an individual document knowing that the word 'the' appears in it.  \n",
    "\n",
    "$$\n",
    "\\mbox{idf}(\\mbox{word})\n",
    "=\n",
    "\\log\\left(\\frac{\\mbox{# of documents}}{\\mbox{# of documents containing the word + 1}}\\right)\n",
    "$$\n",
    "\n",
    "Notes on IDF calculation\n",
    "\n",
    "- If a given word doesn't appear in any documents, the denominator in the equation above would be zero, so it is best to add 1 to the denominator, as indicated above.   \n",
    "- The denominator is *NOT* necessarily equivalent to Term Frequency, which is the total number of times the word appears in a corpus. When a word appears in a document more than once, these numbers will differ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute IDF for the titles of the news ariticles. We need:  \n",
    "\n",
    "- number of occurrences of the word (denominator)   \n",
    "- total number of documents (numerator) \n",
    "\n",
    "First, let's compute IDF for a single word to see how it's done.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: india\n",
      "# of occurrences:  13\n",
      "# of documents:  99\n",
      "idf =  1.9560625205193314\n"
     ]
    }
   ],
   "source": [
    "print(\"word: india\")\n",
    "denom = sum([1 for doc in df.cleaned if 'india' in doc])\n",
    "print(\"# of occurrences: \", denom)\n",
    "num = len(df.cleaned)\n",
    "print(\"# of documents: \", num)\n",
    "idf = np.log(num/(denom+1))\n",
    "print(\"idf = \", idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the IDF function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(word):\n",
    "    n_occurrences = sum([1 for doc in df.cleaned if word in doc])\n",
    "    n_docs = len(df.cleaned)\n",
    "    idf = np.log(n_docs/n_occurrences)\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to do it for all words in our corpus. To do that, we need:  \n",
    "\n",
    "- list of unique words  \n",
    "- remove the words with a count of only 1 (stored in tf_df\n",
    "- assign words to a dataframe  \n",
    "- calculate the idf for each word  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique Words\n",
    "\n",
    "- join all docs together into corpus as single document\n",
    "- split corpus into single words & convert to list of words\n",
    "- keep only a single, unique value for each word. (dedup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = pd.Series(' '.join(df.cleaned).split()).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only words whose frequency is > 1\n",
    "unique_words = list(set(unique_words).intersection(set(tf_df.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- add to a dataframe\n",
    "- apply idf() to compute IDF for each unique word\n",
    "- sort idf_df for quick peeks into the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_count</th>\n",
       "      <th>tf</th>\n",
       "      <th>augmented_frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>india</th>\n",
       "      <td>12</td>\n",
       "      <td>0.036474</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>report</th>\n",
       "      <td>7</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lose</th>\n",
       "      <td>5</td>\n",
       "      <td>0.015198</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case</th>\n",
       "      <td>5</td>\n",
       "      <td>0.015198</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>old</th>\n",
       "      <td>5</td>\n",
       "      <td>0.015198</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        raw_count        tf  augmented_frequency\n",
       "word                                            \n",
       "india          12  0.036474             1.000000\n",
       "report          7  0.021277             0.583333\n",
       "lose            5  0.015198             0.416667\n",
       "case            5  0.015198             0.416667\n",
       "old             5  0.015198             0.416667"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put in a dataframe\n",
    "idf_df = pd.DataFrame(dict(word = unique_words))\\\n",
    "    .assign(idf = lambda df: df.word.apply(idf))\\\n",
    "    .set_index('word').sort_values(by='idf', ascending = False)\n",
    "\n",
    "tfidf_df = idf_df.join(tf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>rakul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>alongsid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ajay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>indra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>kumar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>report</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index         0\n",
       "0      0     rakul\n",
       "1      1      star\n",
       "2      2  alongsid\n",
       "3      3      ajay\n",
       "4      4     indra\n",
       "5      5     kumar\n",
       "6      6      film\n",
       "7      7    report"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll caclulate the tf-idf value for every word across every document\n",
    "tfs = []\n",
    "\n",
    "documents = dict(df.cleaned)\n",
    "for doc, text in documents.items():\n",
    "    tf_df = pd.Series(text.split())\n",
    "\n",
    "# documents.items()\n",
    "tf_df.reset_index()\n",
    "\n",
    "# .set_axis(['index','word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by iterating over all the documents\n",
    "for doc, text in documents.items():\n",
    "    # We'll make a data frame that contains the tf for every word in every document\n",
    "    tf_df = (pd.Series(text.split())\n",
    "          .value_counts()\n",
    "          .reset_index()\n",
    "          .set_axis(['word', 'raw_count'], axis=1, inplace=False)\n",
    "          .assign(tf=lambda df: df.raw_count / df.shape[0])\n",
    "          .drop(columns='raw_count')\n",
    "          .assign(doc=doc))\n",
    "    # Then add that data frame to our list\n",
    "    tfs.append(tf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[      word        tf  doc\n",
       " 0  product  0.111111    0\n",
       " 1   gaffar  0.111111    0\n",
       " 2   market  0.111111    0\n",
       " 3     lakh  0.111111    0\n",
       " 4     fake  0.111111    0\n",
       " 5   xiaomi  0.111111    0\n",
       " 6     seiz  0.111111    0\n",
       " 7    worth  0.111111    0\n",
       " 8    delhi  0.111111    0,      word        tf  doc\n",
       " 0     win  0.142857    1\n",
       " 1  restor  0.142857    1\n",
       " 2    musk  0.142857    1\n",
       " 3   human  0.142857    1\n",
       " 4    case  0.142857    1\n",
       " 5   faith  0.142857    1\n",
       " 6   defam  0.142857    1]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.concat(tfs)\\\n",
    "            .assign(idf = lambda tf_df: tf_df.word.apply(idf))\\\n",
    "            .assign(tf_idf = lambda tf_df: tf_df.idf * tf_df.tf)\\\n",
    "            .drop(columns=['tf', 'idf'])\\\n",
    "            .sort_values(by='tf_idf', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>doc</th>\n",
       "      <th>tf_idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>game</td>\n",
       "      <td>44</td>\n",
       "      <td>1.531707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>industri</td>\n",
       "      <td>5</td>\n",
       "      <td>1.148780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>call</td>\n",
       "      <td>49</td>\n",
       "      <td>1.114849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>onion</td>\n",
       "      <td>8</td>\n",
       "      <td>0.919024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bengaluru</td>\n",
       "      <td>8</td>\n",
       "      <td>0.919024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  doc    tf_idf\n",
       "0       game   44  1.531707\n",
       "0   industri    5  1.148780\n",
       "0       call   49  1.114849\n",
       "0      onion    8  0.919024\n",
       "1  bengaluru    8  0.919024"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>doc</th>\n",
       "      <th>tf_idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>game</td>\n",
       "      <td>44</td>\n",
       "      <td>1.531707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>industri</td>\n",
       "      <td>5</td>\n",
       "      <td>1.148780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>call</td>\n",
       "      <td>49</td>\n",
       "      <td>1.114849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>onion</td>\n",
       "      <td>8</td>\n",
       "      <td>0.919024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bengaluru</td>\n",
       "      <td>8</td>\n",
       "      <td>0.919024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>surg</td>\n",
       "      <td>8</td>\n",
       "      <td>0.919024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>women</td>\n",
       "      <td>94</td>\n",
       "      <td>0.874127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>price</td>\n",
       "      <td>8</td>\n",
       "      <td>0.780395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>slab</td>\n",
       "      <td>11</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watchdog</td>\n",
       "      <td>63</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gst</td>\n",
       "      <td>11</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>consid</td>\n",
       "      <td>11</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mislead</td>\n",
       "      <td>63</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hungari</td>\n",
       "      <td>63</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shaheen</td>\n",
       "      <td>91</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recess</td>\n",
       "      <td>13</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>softbank</td>\n",
       "      <td>70</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>institut</td>\n",
       "      <td>70</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tokyo</td>\n",
       "      <td>70</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acceler</td>\n",
       "      <td>70</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ericsson</td>\n",
       "      <td>71</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raghuram</td>\n",
       "      <td>13</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>password</td>\n",
       "      <td>58</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>growth</td>\n",
       "      <td>13</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bad</td>\n",
       "      <td>91</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wozniacki</td>\n",
       "      <td>39</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>carolin</td>\n",
       "      <td>39</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>retir</td>\n",
       "      <td>39</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>upcom</td>\n",
       "      <td>39</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good</td>\n",
       "      <td>91</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weather</td>\n",
       "      <td>91</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>depress</td>\n",
       "      <td>91</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>season</td>\n",
       "      <td>44</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resolv</td>\n",
       "      <td>71</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>juventu</td>\n",
       "      <td>44</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>youtub</td>\n",
       "      <td>50</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rewind</td>\n",
       "      <td>50</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dislik</td>\n",
       "      <td>50</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hour</td>\n",
       "      <td>50</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>leak</td>\n",
       "      <td>58</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>microsoft</td>\n",
       "      <td>58</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>corrupt</td>\n",
       "      <td>71</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>custom</td>\n",
       "      <td>58</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pay</td>\n",
       "      <td>71</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>misbehav</td>\n",
       "      <td>82</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agre</td>\n",
       "      <td>71</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>crazi</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>flirt</td>\n",
       "      <td>81</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>masayoshi</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>father</td>\n",
       "      <td>81</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>catherin</td>\n",
       "      <td>81</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wife</td>\n",
       "      <td>81</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>law</td>\n",
       "      <td>81</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>srk</td>\n",
       "      <td>82</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metoo</td>\n",
       "      <td>82</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>untouch</td>\n",
       "      <td>82</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>littl</td>\n",
       "      <td>3</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>somebodi</td>\n",
       "      <td>82</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>probe</td>\n",
       "      <td>71</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  doc    tf_idf\n",
       "0       game   44  1.531707\n",
       "0   industri    5  1.148780\n",
       "0       call   49  1.114849\n",
       "0      onion    8  0.919024\n",
       "1  bengaluru    8  0.919024\n",
       "2       surg    8  0.919024\n",
       "0      women   94  0.874127\n",
       "3      price    8  0.780395\n",
       "5       slab   11  0.765853\n",
       "0   watchdog   63  0.765853\n",
       "2        gst   11  0.765853\n",
       "0     consid   11  0.765853\n",
       "3    mislead   63  0.765853\n",
       "5    hungari   63  0.765853\n",
       "0    shaheen   91  0.765853\n",
       "1     recess   13  0.765853\n",
       "0   softbank   70  0.765853\n",
       "1   institut   70  0.765853\n",
       "2      tokyo   70  0.765853\n",
       "4    acceler   70  0.765853\n",
       "0   ericsson   71  0.765853\n",
       "0   raghuram   13  0.765853\n",
       "3   password   58  0.765853\n",
       "5     growth   13  0.765853\n",
       "1        bad   91  0.765853\n",
       "0  wozniacki   39  0.765853\n",
       "1    carolin   39  0.765853\n",
       "2      retir   39  0.765853\n",
       "4      upcom   39  0.765853\n",
       "4       good   91  0.765853\n",
       "3    weather   91  0.765853\n",
       "2    depress   91  0.765853\n",
       "2     season   44  0.765853\n",
       "2     resolv   71  0.765853\n",
       "4    juventu   44  0.765853\n",
       "0     youtub   50  0.765853\n",
       "1     rewind   50  0.765853\n",
       "2     dislik   50  0.765853\n",
       "5       hour   50  0.765853\n",
       "0       leak   58  0.765853\n",
       "2  microsoft   58  0.765853\n",
       "1    corrupt   71  0.765853\n",
       "4     custom   58  0.765853\n",
       "3        pay   71  0.765853\n",
       "4   misbehav   82  0.765853\n",
       "4       agre   71  0.765853\n",
       "3      crazi    3  0.765853\n",
       "0      flirt   81  0.765853\n",
       "4  masayoshi    3  0.765853\n",
       "1     father   81  0.765853\n",
       "2   catherin   81  0.765853\n",
       "4       wife   81  0.765853\n",
       "5        law   81  0.765853\n",
       "0        srk   82  0.765853\n",
       "2      metoo   82  0.765853\n",
       "3    untouch   82  0.765853\n",
       "5      littl    3  0.765853\n",
       "5   somebodi   82  0.765853\n",
       "5      probe   71  0.765853"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at importances based on tf_idf values\n",
    "\n",
    "tfidf_df.sort_values('tf_idf', ascending = False)\n",
    "reduced_df = tfidf_df[tfidf_df.tf_idf > .75]\n",
    "reduced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidfs = tfidf.fit_transform(documents.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a sparse matrix:  \n",
    "- a matrix with more 0s than anything else. \n",
    "- Numpy has a special type that makes some manipulations and operations faster on sparse matrices.\n",
    "\n",
    "Becuase our data set is pretty small, we can convert our sparse matrix to a regular one, aka 'dense matrix', and put everything in a dataframe. If our data were larger, the operation below might take much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acceler</th>\n",
       "      <th>account</th>\n",
       "      <th>acquir</th>\n",
       "      <th>across</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actress</th>\n",
       "      <th>age</th>\n",
       "      <th>agre</th>\n",
       "      <th>air</th>\n",
       "      <th>...</th>\n",
       "      <th>wrong</th>\n",
       "      <th>xiaomi</th>\n",
       "      <th>yard</th>\n",
       "      <th>year</th>\n",
       "      <th>yet</th>\n",
       "      <th>yohan</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>yuvraj</th>\n",
       "      <th>zilingo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.342884</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 574 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   acceler  account  acquir  across  action  actor  actress  age  agre  air  \\\n",
       "0      0.0      0.0     0.0     0.0     0.0    0.0      0.0  0.0   0.0  0.0   \n",
       "1      0.0      0.0     0.0     0.0     0.0    0.0      0.0  0.0   0.0  0.0   \n",
       "2      0.0      0.0     0.0     0.0     0.0    0.0      0.0  0.0   0.0  0.0   \n",
       "3      0.0      0.0     0.0     0.0     0.0    0.0      0.0  0.0   0.0  0.0   \n",
       "4      0.0      0.0     0.0     0.0     0.0    0.0      0.0  0.0   0.0  0.0   \n",
       "\n",
       "    ...     wrong    xiaomi  yard  year  yet  yohan  youtub   yr  yuvraj  \\\n",
       "0   ...       0.0  0.342884   0.0   0.0  0.0    0.0     0.0  0.0     0.0   \n",
       "1   ...       0.0  0.000000   0.0   0.0  0.0    0.0     0.0  0.0     0.0   \n",
       "2   ...       0.0  0.000000   0.0   0.0  0.0    0.0     0.0  0.0     0.0   \n",
       "3   ...       0.0  0.000000   0.0   0.0  0.0    0.0     0.0  0.0     0.0   \n",
       "4   ...       0.0  0.000000   0.0   0.0  0.0    0.0     0.0  0.0     0.0   \n",
       "\n",
       "   zilingo  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      0.0  \n",
       "\n",
       "[5 rows x 574 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df = pd.DataFrame(tfidfs.todense(), columns=tfidf.get_feature_names())\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>word</th>\n",
       "      <th>game</th>\n",
       "      <th>industri</th>\n",
       "      <th>call</th>\n",
       "      <th>onion</th>\n",
       "      <th>bengaluru</th>\n",
       "      <th>surg</th>\n",
       "      <th>women</th>\n",
       "      <th>price</th>\n",
       "      <th>slab</th>\n",
       "      <th>watchdog</th>\n",
       "      <th>...</th>\n",
       "      <th>father</th>\n",
       "      <th>catherin</th>\n",
       "      <th>wife</th>\n",
       "      <th>law</th>\n",
       "      <th>srk</th>\n",
       "      <th>metoo</th>\n",
       "      <th>untouch</th>\n",
       "      <th>littl</th>\n",
       "      <th>somebodi</th>\n",
       "      <th>probe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tf_idf</th>\n",
       "      <td>1.531707</td>\n",
       "      <td>1.14878</td>\n",
       "      <td>1.114849</td>\n",
       "      <td>0.919024</td>\n",
       "      <td>0.919024</td>\n",
       "      <td>0.919024</td>\n",
       "      <td>0.874127</td>\n",
       "      <td>0.780395</td>\n",
       "      <td>0.765853</td>\n",
       "      <td>0.765853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.765853</td>\n",
       "      <td>0.765853</td>\n",
       "      <td>0.765853</td>\n",
       "      <td>0.765853</td>\n",
       "      <td>0.765853</td>\n",
       "      <td>0.765853</td>\n",
       "      <td>0.765853</td>\n",
       "      <td>0.765853</td>\n",
       "      <td>0.765853</td>\n",
       "      <td>0.765853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "word        game  industri      call     onion  bengaluru      surg     women  \\\n",
       "tf_idf  1.531707   1.14878  1.114849  0.919024   0.919024  0.919024  0.874127   \n",
       "\n",
       "word       price      slab  watchdog    ...       father  catherin      wife  \\\n",
       "tf_idf  0.780395  0.765853  0.765853    ...     0.765853  0.765853  0.765853   \n",
       "\n",
       "word         law       srk     metoo   untouch     littl  somebodi     probe  \n",
       "tf_idf  0.765853  0.765853  0.765853  0.765853  0.765853  0.765853  0.765853  \n",
       "\n",
       "[1 rows x 59 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_df[['word', 'tf_idf']].set_index('word').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>original</th>\n",
       "      <th>normalized</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Fake Xiaomi products worth ₹13 lakh seized fro...</td>\n",
       "      <td>fake xiaomi products worth    lakh seized from...</td>\n",
       "      <td>fake xiaomi product worth lakh seiz from delhi...</td>\n",
       "      <td>fake xiaomi product worth lakh seized from del...</td>\n",
       "      <td>fake xiaomi product worth lakh seiz delhi gaff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>My faith in humanity is restored: Musk after w...</td>\n",
       "      <td>faith  humanity  restored  musk after winning ...</td>\n",
       "      <td>faith human restor musk after win defam case</td>\n",
       "      <td>faith humanity restored musk after winning def...</td>\n",
       "      <td>faith human restor musk win defam case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>I'll take it on the chin: Cave explorer after ...</td>\n",
       "      <td>take   the chin  cave explorer after losing ca...</td>\n",
       "      <td>take the chin cave explor after lose case agai...</td>\n",
       "      <td>take the chin cave explorer after losing case ...</td>\n",
       "      <td>take chin cave explor lose case elon musk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>We are the same animal, we are both a little c...</td>\n",
       "      <td>are the same animal   are both  little crazy  ...</td>\n",
       "      <td>are the same anim are both littl crazi masayos...</td>\n",
       "      <td>are the same animal are both little crazy masa...</td>\n",
       "      <td>anim littl crazi masayoshi son jack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>23-yr-old finds Airtel app bug that could have...</td>\n",
       "      <td>old finds airtel app bug that could have expos...</td>\n",
       "      <td>old find airtel app bug that could have expos ...</td>\n",
       "      <td>old find airtel app bug that could have expose...</td>\n",
       "      <td>old find airtel app bug could expos data crore...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                           original  \\\n",
       "0  business  Fake Xiaomi products worth ₹13 lakh seized fro...   \n",
       "1  business  My faith in humanity is restored: Musk after w...   \n",
       "2  business  I'll take it on the chin: Cave explorer after ...   \n",
       "3  business  We are the same animal, we are both a little c...   \n",
       "4  business  23-yr-old finds Airtel app bug that could have...   \n",
       "\n",
       "                                          normalized  \\\n",
       "0  fake xiaomi products worth    lakh seized from...   \n",
       "1  faith  humanity  restored  musk after winning ...   \n",
       "2  take   the chin  cave explorer after losing ca...   \n",
       "3  are the same animal   are both  little crazy  ...   \n",
       "4  old finds airtel app bug that could have expos...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  fake xiaomi product worth lakh seiz from delhi...   \n",
       "1       faith human restor musk after win defam case   \n",
       "2  take the chin cave explor after lose case agai...   \n",
       "3  are the same anim are both littl crazi masayos...   \n",
       "4  old find airtel app bug that could have expos ...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  fake xiaomi product worth lakh seized from del...   \n",
       "1  faith humanity restored musk after winning def...   \n",
       "2  take the chin cave explorer after losing case ...   \n",
       "3  are the same animal are both little crazy masa...   \n",
       "4  old find airtel app bug that could have expose...   \n",
       "\n",
       "                                             cleaned  \n",
       "0  fake xiaomi product worth lakh seiz delhi gaff...  \n",
       "1             faith human restor musk win defam case  \n",
       "2          take chin cave explor lose case elon musk  \n",
       "3                anim littl crazi masayoshi son jack  \n",
       "4  old find airtel app bug could expos data crore...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = prep_news_articles('title','category')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(df.cleaned)\n",
    "y = df.category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    business\n",
       "1    business\n",
       "2    business\n",
       "3    business\n",
       "4    business\n",
       "Name: category, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(dict(actual=y_train))\n",
    "test = pd.DataFrame(dict(actual=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(criterion='entropy', splitter='best', \n",
    "                            max_depth=5, min_samples_split=2,\n",
    "                           min_samples_leaf=1, random_state=123)\\\n",
    "            .fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['predicted'] = dt.predict(X_train)\n",
    "test['predicted'] = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>technology</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>technology</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>business</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           actual predicted\n",
       "85  entertainment    sports\n",
       "81  entertainment    sports\n",
       "49     technology    sports\n",
       "52     technology  business\n",
       "7        business    sports"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 41.77%\n",
      "---\n",
      "Confusion Matrix\n",
      "actual         business  entertainment  sports  technology\n",
      "predicted                                                 \n",
      "business              3              0       0           0\n",
      "entertainment         0              4       0           0\n",
      "sports               15             16      20          14\n",
      "technology            1              0       0           6\n",
      "---\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       1.00      0.16      0.27        19\n",
      "entertainment       1.00      0.20      0.33        20\n",
      "       sports       0.31      1.00      0.47        20\n",
      "   technology       0.86      0.30      0.44        20\n",
      "\n",
      "    micro avg       0.42      0.42      0.42        79\n",
      "    macro avg       0.79      0.41      0.38        79\n",
      " weighted avg       0.79      0.42      0.38        79\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.2%}'.format(accuracy_score(train.actual, train.predicted)))\n",
    "print('---')\n",
    "print('Confusion Matrix')\n",
    "print(pd.crosstab(train.predicted, train.actual))\n",
    "print('---')\n",
    "print(classification_report(train.actual, train.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 25.00%\n",
      "---\n",
      "Confusion Matrix\n",
      "actual     business  entertainment  sports  technology\n",
      "predicted                                             \n",
      "business          0              0       0           2\n",
      "sports            5              5       5           3\n",
      "---\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.00      0.00      0.00         5\n",
      "entertainment       0.00      0.00      0.00         5\n",
      "       sports       0.28      1.00      0.43         5\n",
      "   technology       0.00      0.00      0.00         5\n",
      "\n",
      "    micro avg       0.25      0.25      0.25        20\n",
      "    macro avg       0.07      0.25      0.11        20\n",
      " weighted avg       0.07      0.25      0.11        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maggiegiust/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.2%}'.format(accuracy_score(test.actual, test.predicted)))\n",
    "print('---')\n",
    "print('Confusion Matrix')\n",
    "print(pd.crosstab(test.predicted, test.actual))\n",
    "print('---')\n",
    "print(classification_report(test.actual, test.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maggiegiust/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(probability=True)\n",
    "svc = svc.fit(X_train, y_train)\n",
    "\n",
    "train['predicted'] = svc.predict(X_train)\n",
    "test['predicted'] = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Confusion Matrix\n",
      "actual         business  entertainment  sports  technology\n",
      "predicted                                                 \n",
      "entertainment         2             20       0           0\n",
      "sports                2              0      20           0\n",
      "technology           15              0       0          20\n",
      "---\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.00      0.00      0.00        19\n",
      "entertainment       0.91      1.00      0.95        20\n",
      "       sports       0.91      1.00      0.95        20\n",
      "   technology       0.57      1.00      0.73        20\n",
      "\n",
      "    micro avg       0.76      0.76      0.76        79\n",
      "    macro avg       0.60      0.75      0.66        79\n",
      " weighted avg       0.60      0.76      0.67        79\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('---')\n",
    "print('Confusion Matrix')\n",
    "print(pd.crosstab(train.predicted, train.actual))\n",
    "print('---')\n",
    "print(classification_report(train.actual, train.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.00%\n",
      "---\n",
      "Confusion Matrix\n",
      "actual         business  entertainment  sports  technology\n",
      "predicted                                                 \n",
      "entertainment         0              1       0           1\n",
      "sports                2              0       5           0\n",
      "technology            3              4       0           4\n",
      "---\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.00      0.00      0.00         5\n",
      "entertainment       0.50      0.20      0.29         5\n",
      "       sports       0.71      1.00      0.83         5\n",
      "   technology       0.36      0.80      0.50         5\n",
      "\n",
      "    micro avg       0.50      0.50      0.50        20\n",
      "    macro avg       0.39      0.50      0.40        20\n",
      " weighted avg       0.39      0.50      0.40        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.2%}'.format(accuracy_score(test.actual, test.predicted)))\n",
    "print('---')\n",
    "print('Confusion Matrix')\n",
    "print(pd.crosstab(test.predicted, test.actual))\n",
    "print('---')\n",
    "print(classification_report(test.actual, test.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestClassifier in module sklearn.ensemble.forest:\n",
      "\n",
      "class RandomForestClassifier(ForestClassifier)\n",
      " |  A random forest classifier.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of decision tree\n",
      " |  classifiers on various sub-samples of the dataset and uses averaging to\n",
      " |  improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is always the same as the original\n",
      " |  input sample size but the samples are drawn with replacement if\n",
      " |  `bootstrap=True` (default).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : integer, optional (default=10)\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |         The default value of ``n_estimators`` will change from 10 in\n",
      " |         version 0.20 to 100 in version 0.22.\n",
      " |  \n",
      " |  criterion : string, optional (default=\"gini\")\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  max_depth : integer or None, optional (default=None)\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int, float, optional (default=2)\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int, float, optional (default=1)\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : int, float, string or None, optional (default=\"auto\")\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int or None, optional (default=None)\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, optional (default=0.)\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, (default=1e-7)\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  \n",
      " |  bootstrap : boolean, optional (default=True)\n",
      " |      Whether bootstrap samples are used when building trees.\n",
      " |  \n",
      " |  oob_score : bool (default=False)\n",
      " |      Whether to use out-of-bag samples to estimate\n",
      " |      the generalization accuracy.\n",
      " |  \n",
      " |  n_jobs : int or None, optional (default=None)\n",
      " |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  verbose : int, optional (default=0)\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, optional (default=False)\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  class_weight : dict, list of dicts, \"balanced\", \"balanced_subsample\" or     None, optional (default=None)\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      " |      weights are computed based on the bootstrap sample for every tree\n",
      " |      grown.\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimators_ : list of DecisionTreeClassifier\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : array of shape = [n_classes] or a list of such arrays\n",
      " |      The classes labels (single output problem), or a list of arrays of\n",
      " |      class labels (multi-output problem).\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes (single output problem), or a list containing the\n",
      " |      number of classes for each output (multi-output problem).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  feature_importances_ : array of shape = [n_features]\n",
      " |      The feature importances (the higher, the more important the feature).\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |  \n",
      " |  oob_decision_function_ : array of shape = [n_samples, n_classes]\n",
      " |      Decision function computed with out-of-bag estimate on the training\n",
      " |      set. If n_estimators is small it might be possible that a data point\n",
      " |      was never left out during the bootstrap. In this case,\n",
      " |      `oob_decision_function_` might contain NaN.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  \n",
      " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
      " |  ...                              random_state=0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      " |              max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
      " |              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      " |              min_samples_leaf=1, min_samples_split=2,\n",
      " |              min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      " |              oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      " |  >>> print(clf.feature_importances_)\n",
      " |  [0.14205973 0.76664038 0.0282433  0.06305659]\n",
      " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  DecisionTreeClassifier, ExtraTreesClassifier\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestClassifier\n",
      " |      ForestClassifier\n",
      " |      abc.NewBase\n",
      " |      BaseForest\n",
      " |      abc.NewBase\n",
      " |      sklearn.ensemble.base.BaseEnsemble\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators='warn', criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestClassifier:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is a vote by the trees in\n",
      " |      the forest, weighted by their probability estimates. That is,\n",
      " |      the predicted class is the one with highest mean probability\n",
      " |      estimate across the trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the log of the mean predicted class probabilities of the trees in the\n",
      " |      forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute `classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample are computed as\n",
      " |      the mean predicted class probabilities of the trees in the forest. The\n",
      " |      class probability of a single tree is the fraction of samples of the same\n",
      " |      class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute `classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      " |          Return a node indicator matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |      \n",
      " |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples] or None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances (the higher, the more important the\n",
      " |         feature).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array, shape = [n_features]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Returns the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Returns iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Returns the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 100, max_depth = 3, min_samples_split=5)\n",
    "rf = rf.fit(X_train, y_train)\n",
    "\n",
    "train['predicted'] = rf.predict(X_train)\n",
    "test['predicted'] = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.94%\n",
      "---\n",
      "Confusion Matrix\n",
      "actual         business  entertainment  sports  technology\n",
      "predicted                                                 \n",
      "business             16              0       0           0\n",
      "entertainment         1             20       0           0\n",
      "sports                1              0      19           0\n",
      "technology            1              0       1          20\n",
      "---\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       1.00      0.84      0.91        19\n",
      "entertainment       0.95      1.00      0.98        20\n",
      "       sports       0.95      0.95      0.95        20\n",
      "   technology       0.91      1.00      0.95        20\n",
      "\n",
      "    micro avg       0.95      0.95      0.95        79\n",
      "    macro avg       0.95      0.95      0.95        79\n",
      " weighted avg       0.95      0.95      0.95        79\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.2%}'.format(accuracy_score(train.actual, train.predicted)))\n",
    "print('---')\n",
    "print('Confusion Matrix')\n",
    "print(pd.crosstab(train.predicted, train.actual))\n",
    "print('---')\n",
    "print(classification_report(train.actual, train.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 30.00%\n",
      "---\n",
      "Confusion Matrix\n",
      "actual         business  entertainment  sports  technology\n",
      "predicted                                                 \n",
      "business              0              0       0           2\n",
      "entertainment         1              1       0           1\n",
      "sports                1              0       3           0\n",
      "technology            3              4       2           2\n",
      "---\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.00      0.00      0.00         5\n",
      "entertainment       0.33      0.20      0.25         5\n",
      "       sports       0.75      0.60      0.67         5\n",
      "   technology       0.18      0.40      0.25         5\n",
      "\n",
      "    micro avg       0.30      0.30      0.30        20\n",
      "    macro avg       0.32      0.30      0.29        20\n",
      " weighted avg       0.32      0.30      0.29        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.2%}'.format(accuracy_score(test.actual, test.predicted)))\n",
    "print('---')\n",
    "print('Confusion Matrix')\n",
    "print(pd.crosstab(test.predicted, test.actual))\n",
    "print('---')\n",
    "print(classification_report(test.actual, test.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "know        0.003648\n",
       "vietnam     0.003650\n",
       "titl        0.003661\n",
       "thrash      0.003662\n",
       "model       0.003687\n",
       "googl       0.003688\n",
       "teammat     0.003701\n",
       "map         0.003734\n",
       "hour        0.003744\n",
       "csa         0.003747\n",
       "musk        0.003759\n",
       "lionel      0.003775\n",
       "resolv      0.003799\n",
       "vijay       0.003812\n",
       "big         0.003813\n",
       "gcx         0.003844\n",
       "postpon     0.003882\n",
       "catch       0.003883\n",
       "hacker      0.003892\n",
       "go          0.003911\n",
       "singh       0.003918\n",
       "good        0.003945\n",
       "wood        0.003958\n",
       "war         0.003959\n",
       "kid         0.004000\n",
       "across      0.004004\n",
       "decemb      0.004065\n",
       "dribbl      0.004078\n",
       "human       0.004148\n",
       "issu        0.004154\n",
       "              ...   \n",
       "kevin       0.006747\n",
       "befor       0.006775\n",
       "harass      0.007016\n",
       "au          0.007227\n",
       "indi        0.007461\n",
       "site        0.007740\n",
       "cancer      0.007810\n",
       "old         0.007862\n",
       "first       0.008076\n",
       "kamal       0.008152\n",
       "airtel      0.008472\n",
       "wa          0.008565\n",
       "pak         0.009519\n",
       "take        0.009736\n",
       "month       0.009864\n",
       "lose        0.010421\n",
       "test        0.011665\n",
       "rajan       0.011923\n",
       "india       0.013170\n",
       "star        0.013188\n",
       "dope        0.014357\n",
       "case        0.015620\n",
       "score       0.015922\n",
       "chang       0.017528\n",
       "user        0.018334\n",
       "report      0.018731\n",
       "research    0.019801\n",
       "seri        0.023424\n",
       "appl        0.023524\n",
       "film        0.025050\n",
       "Length: 100, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(dict(zip(tfidf.get_feature_names(), rf.feature_importances_))).sort_values().tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acceler',\n",
       " 'account',\n",
       " 'acquir',\n",
       " 'across',\n",
       " 'action',\n",
       " 'actor',\n",
       " 'actress',\n",
       " 'age',\n",
       " 'agre',\n",
       " 'air',\n",
       " 'airport',\n",
       " 'airtel',\n",
       " 'ajay',\n",
       " 'alia',\n",
       " 'alongsid',\n",
       " 'also',\n",
       " 'analytica',\n",
       " 'ananya',\n",
       " 'anderson',\n",
       " 'anim',\n",
       " 'ankiti',\n",
       " 'app',\n",
       " 'appl',\n",
       " 'arjun',\n",
       " 'around',\n",
       " 'associ',\n",
       " 'athlet',\n",
       " 'attempt',\n",
       " 'au',\n",
       " 'auto',\n",
       " 'avail',\n",
       " 'ayushmann',\n",
       " 'back',\n",
       " 'bad',\n",
       " 'bairstow',\n",
       " 'bajaj',\n",
       " 'balakot',\n",
       " 'ball',\n",
       " 'ban',\n",
       " 'bank',\n",
       " 'banker',\n",
       " 'battl',\n",
       " 'bcci',\n",
       " 'beast',\n",
       " 'beat',\n",
       " 'becom',\n",
       " 'bed',\n",
       " 'befor',\n",
       " 'bengaluru',\n",
       " 'best',\n",
       " 'bharti',\n",
       " 'big',\n",
       " 'bikini',\n",
       " 'birla',\n",
       " 'black',\n",
       " 'blake',\n",
       " 'bmw',\n",
       " 'board',\n",
       " 'bodi',\n",
       " 'boe',\n",
       " 'bose',\n",
       " 'bowl',\n",
       " 'break',\n",
       " 'breast',\n",
       " 'budget',\n",
       " 'bug',\n",
       " 'burn',\n",
       " 'burnley',\n",
       " 'cabl',\n",
       " 'call',\n",
       " 'cambridg',\n",
       " 'campaign',\n",
       " 'cancer',\n",
       " 'carolin',\n",
       " 'case',\n",
       " 'catch',\n",
       " 'catherin',\n",
       " 'cave',\n",
       " 'celebr',\n",
       " 'ceo',\n",
       " 'certif',\n",
       " 'chang',\n",
       " 'charg',\n",
       " 'chest',\n",
       " 'chhalaang',\n",
       " 'chin',\n",
       " 'china',\n",
       " 'chiranjeevi',\n",
       " 'choic',\n",
       " 'cloud',\n",
       " 'coai',\n",
       " 'come',\n",
       " 'commentari',\n",
       " 'commun',\n",
       " 'compani',\n",
       " 'compon',\n",
       " 'confirm',\n",
       " 'congratul',\n",
       " 'consid',\n",
       " 'contest',\n",
       " 'control',\n",
       " 'corp',\n",
       " 'corrupt',\n",
       " 'could',\n",
       " 'crazi',\n",
       " 'creat',\n",
       " 'cricket',\n",
       " 'crisi',\n",
       " 'cristiano',\n",
       " 'criticis',\n",
       " 'crore',\n",
       " 'crowd',\n",
       " 'csa',\n",
       " 'cup',\n",
       " 'custom',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'dabur',\n",
       " 'dad',\n",
       " 'danger',\n",
       " 'data',\n",
       " 'day',\n",
       " 'deceiv',\n",
       " 'decemb',\n",
       " 'decis',\n",
       " 'defam',\n",
       " 'definit',\n",
       " 'delhi',\n",
       " 'depress',\n",
       " 'develop',\n",
       " 'dharma',\n",
       " 'diagnos',\n",
       " 'diaper',\n",
       " 'die',\n",
       " 'director',\n",
       " 'dislik',\n",
       " 'dismiss',\n",
       " 'dope',\n",
       " 'dribbl',\n",
       " 'dubiou',\n",
       " 'dwayn',\n",
       " 'econom',\n",
       " 'economi',\n",
       " 'edit',\n",
       " 'effigi',\n",
       " 'either',\n",
       " 'elon',\n",
       " 'encount',\n",
       " 'end',\n",
       " 'eng',\n",
       " 'entertain',\n",
       " 'ericsson',\n",
       " 'even',\n",
       " 'evil',\n",
       " 'explor',\n",
       " 'expos',\n",
       " 'express',\n",
       " 'face',\n",
       " 'facebook',\n",
       " 'faf',\n",
       " 'fail',\n",
       " 'faith',\n",
       " 'fake',\n",
       " 'fan',\n",
       " 'father',\n",
       " 'faulti',\n",
       " 'fdi',\n",
       " 'fearlessli',\n",
       " 'field',\n",
       " 'fifa',\n",
       " 'film',\n",
       " 'final',\n",
       " 'find',\n",
       " 'fine',\n",
       " 'firm',\n",
       " 'first',\n",
       " 'fix',\n",
       " 'flirt',\n",
       " 'floor',\n",
       " 'follow',\n",
       " 'forc',\n",
       " 'foreign',\n",
       " 'four',\n",
       " 'free',\n",
       " 'freedom',\n",
       " 'front',\n",
       " 'ftc',\n",
       " 'fudg',\n",
       " 'gaffar',\n",
       " 'game',\n",
       " 'ganguli',\n",
       " 'gate',\n",
       " 'gave',\n",
       " 'gcx',\n",
       " 'get',\n",
       " 'gift',\n",
       " 'glass',\n",
       " 'go',\n",
       " 'good',\n",
       " 'googl',\n",
       " 'got',\n",
       " 'govern',\n",
       " 'govt',\n",
       " 'group',\n",
       " 'growth',\n",
       " 'gst',\n",
       " 'haasan',\n",
       " 'hack',\n",
       " 'hacker',\n",
       " 'harass',\n",
       " 'hat',\n",
       " 'hatr',\n",
       " 'hijack',\n",
       " 'hint',\n",
       " 'hit',\n",
       " 'hobart',\n",
       " 'hockey',\n",
       " 'hole',\n",
       " 'home',\n",
       " 'hospit',\n",
       " 'hour',\n",
       " 'hrithik',\n",
       " 'human',\n",
       " 'hungari',\n",
       " 'hyd',\n",
       " 'hyundai',\n",
       " 'idea',\n",
       " 'import',\n",
       " 'inch',\n",
       " 'incom',\n",
       " 'indi',\n",
       " 'india',\n",
       " 'indian',\n",
       " 'indra',\n",
       " 'industri',\n",
       " 'ineffect',\n",
       " 'info',\n",
       " 'institut',\n",
       " 'intellectu',\n",
       " 'interrupt',\n",
       " 'iphon',\n",
       " 'ishaan',\n",
       " 'issu',\n",
       " 'jack',\n",
       " 'jim',\n",
       " 'job',\n",
       " 'journalist',\n",
       " 'journo',\n",
       " 'juic',\n",
       " 'junior',\n",
       " 'juventu',\n",
       " 'kabir',\n",
       " 'kamal',\n",
       " 'kevin',\n",
       " 'khan',\n",
       " 'khatter',\n",
       " 'khurrana',\n",
       " 'kiara',\n",
       " 'kid',\n",
       " 'knock',\n",
       " 'know',\n",
       " 'kohli',\n",
       " 'kumar',\n",
       " 'lakh',\n",
       " 'lamichhan',\n",
       " 'lap',\n",
       " 'lata',\n",
       " 'latif',\n",
       " 'laud',\n",
       " 'launch',\n",
       " 'law',\n",
       " 'laxman',\n",
       " 'leak',\n",
       " 'learn',\n",
       " 'legal',\n",
       " 'lend',\n",
       " 'less',\n",
       " 'lesson',\n",
       " 'level',\n",
       " 'liga',\n",
       " 'like',\n",
       " 'link',\n",
       " 'lionel',\n",
       " 'liquor',\n",
       " 'listen',\n",
       " 'littl',\n",
       " 'local',\n",
       " 'lose',\n",
       " 'lost',\n",
       " 'love',\n",
       " 'macbook',\n",
       " 'mahesh',\n",
       " 'make',\n",
       " 'maldiv',\n",
       " 'manipul',\n",
       " 'map',\n",
       " 'market',\n",
       " 'marri',\n",
       " 'masayoshi',\n",
       " 'match',\n",
       " 'matrimoni',\n",
       " 'matter',\n",
       " 'may',\n",
       " 'meet',\n",
       " 'messi',\n",
       " 'met',\n",
       " 'metoo',\n",
       " 'microsoft',\n",
       " 'million',\n",
       " 'misbehav',\n",
       " 'mislead',\n",
       " 'misogyni',\n",
       " 'miss',\n",
       " 'mit',\n",
       " 'model',\n",
       " 'moment',\n",
       " 'mona',\n",
       " 'money',\n",
       " 'month',\n",
       " 'mother',\n",
       " 'move',\n",
       " 'much',\n",
       " 'mukerji',\n",
       " 'musk',\n",
       " 'name',\n",
       " 'nation',\n",
       " 'nawazuddin',\n",
       " 'nba',\n",
       " 'nehwal',\n",
       " 'netflix',\n",
       " 'new',\n",
       " 'nod',\n",
       " 'nushrat',\n",
       " 'offer',\n",
       " 'offic',\n",
       " 'offici',\n",
       " 'old',\n",
       " 'olymp',\n",
       " 'one',\n",
       " 'onion',\n",
       " 'open',\n",
       " 'owner',\n",
       " 'ownership',\n",
       " 'oye',\n",
       " 'pak',\n",
       " 'panel',\n",
       " 'panipat',\n",
       " 'parent',\n",
       " 'part',\n",
       " 'passion',\n",
       " 'password',\n",
       " 'pay',\n",
       " 'peopl',\n",
       " 'per',\n",
       " 'person',\n",
       " 'pietersen',\n",
       " 'pink',\n",
       " 'plane',\n",
       " 'play',\n",
       " 'player',\n",
       " 'pneumonia',\n",
       " 'polic',\n",
       " 'polit',\n",
       " 'pop',\n",
       " 'postpon',\n",
       " 'potenti',\n",
       " 'power',\n",
       " 'prais',\n",
       " 'pre',\n",
       " 'presid',\n",
       " 'price',\n",
       " 'pro',\n",
       " 'probe',\n",
       " 'problem',\n",
       " 'product',\n",
       " 'properti',\n",
       " 'protest',\n",
       " 'public',\n",
       " 'qatar',\n",
       " 'qualifi',\n",
       " 'question',\n",
       " 'radio',\n",
       " 'raghuram',\n",
       " 'rais',\n",
       " 'rajan',\n",
       " 'rajasthan',\n",
       " 'rajinikanth',\n",
       " 'rajkummar',\n",
       " 'rakul',\n",
       " 'ramdin',\n",
       " 'ramp',\n",
       " 'rani',\n",
       " 'rapper',\n",
       " 'rashid',\n",
       " 'rate',\n",
       " 'readi',\n",
       " 'recess',\n",
       " 'record',\n",
       " 'recov',\n",
       " 'reddi',\n",
       " 'reddit',\n",
       " 'reform',\n",
       " 'regim',\n",
       " 'regul',\n",
       " 'relianc',\n",
       " 'remark',\n",
       " 'report',\n",
       " 'reportedli',\n",
       " 'research',\n",
       " 'resolv',\n",
       " 'respond',\n",
       " 'respons',\n",
       " 'restor',\n",
       " 'retir',\n",
       " 'return',\n",
       " 'reveal',\n",
       " 'rewind',\n",
       " 'richard',\n",
       " 'right',\n",
       " 'ronaldo',\n",
       " 'rule',\n",
       " 'run',\n",
       " 'russia',\n",
       " 'russian',\n",
       " 'sania',\n",
       " 'satnam',\n",
       " 'say',\n",
       " 'scan',\n",
       " 'scarlett',\n",
       " 'scheme',\n",
       " 'school',\n",
       " 'score',\n",
       " 'screen',\n",
       " 'season',\n",
       " 'sebi',\n",
       " 'secret',\n",
       " 'see',\n",
       " 'seek',\n",
       " 'seiz',\n",
       " 'seizur',\n",
       " 'self',\n",
       " 'sell',\n",
       " 'send',\n",
       " 'seri',\n",
       " 'servic',\n",
       " 'set',\n",
       " 'shadow',\n",
       " 'shaheen',\n",
       " 'shoaib',\n",
       " 'shout',\n",
       " 'show',\n",
       " 'singh',\n",
       " 'sister',\n",
       " 'site',\n",
       " 'sitharaman',\n",
       " 'slab',\n",
       " 'slip',\n",
       " 'small',\n",
       " 'softbank',\n",
       " 'softwar',\n",
       " 'solar',\n",
       " 'solut',\n",
       " 'somebodi',\n",
       " 'someon',\n",
       " 'sometim',\n",
       " 'son',\n",
       " 'sona',\n",
       " 'sound',\n",
       " 'sourav',\n",
       " 'south',\n",
       " 'spammer',\n",
       " 'speak',\n",
       " 'spur',\n",
       " 'squad',\n",
       " 'srk',\n",
       " 'stadium',\n",
       " 'stage',\n",
       " 'star',\n",
       " 'start',\n",
       " 'stop',\n",
       " 'streak',\n",
       " 'stream',\n",
       " 'street',\n",
       " 'suffer',\n",
       " 'support',\n",
       " 'surfac',\n",
       " 'surg',\n",
       " 'suspect',\n",
       " 'syama',\n",
       " 'take',\n",
       " 'talk',\n",
       " 'target',\n",
       " 'tax',\n",
       " 'team',\n",
       " 'teammat',\n",
       " 'tech',\n",
       " 'telecom',\n",
       " 'tell',\n",
       " 'terrorist',\n",
       " 'test',\n",
       " 'tharoor',\n",
       " 'thi',\n",
       " 'thrash',\n",
       " 'ticket',\n",
       " 'titl',\n",
       " 'tnpl',\n",
       " 'togeth',\n",
       " 'toilet',\n",
       " 'tokyo',\n",
       " 'tool',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'trade',\n",
       " 'trai',\n",
       " 'trailer',\n",
       " 'tri',\n",
       " 'trick',\n",
       " 'true',\n",
       " 'trump',\n",
       " 'turram',\n",
       " 'tweet',\n",
       " 'two',\n",
       " 'umpir',\n",
       " 'undersea',\n",
       " 'unit',\n",
       " 'univers',\n",
       " 'unseen',\n",
       " 'untouch',\n",
       " 'upcom',\n",
       " 'updat',\n",
       " 'use',\n",
       " 'user',\n",
       " 'varun',\n",
       " 'veri',\n",
       " 'video',\n",
       " 'vietnam',\n",
       " 'vijay',\n",
       " 'virat',\n",
       " 'vivek',\n",
       " 'vivian',\n",
       " 'voic',\n",
       " 'vv',\n",
       " 'wa',\n",
       " 'wait',\n",
       " 'walk',\n",
       " 'want',\n",
       " 'war',\n",
       " 'warn',\n",
       " 'watchdog',\n",
       " 'weather',\n",
       " 'west',\n",
       " 'wet',\n",
       " 'whatsapp',\n",
       " 'widow',\n",
       " 'wife',\n",
       " 'win',\n",
       " 'wit',\n",
       " 'women',\n",
       " 'wood',\n",
       " 'work',\n",
       " 'world',\n",
       " 'worth',\n",
       " 'wozniacki',\n",
       " 'wrld',\n",
       " 'wrong',\n",
       " 'xiaomi',\n",
       " 'yard',\n",
       " 'year',\n",
       " 'yet',\n",
       " 'yohan',\n",
       " 'youtub',\n",
       " 'yr',\n",
       " 'yuvraj',\n",
       " 'zilingo']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.feature_importances_\n",
    "tfidf.get_feature_names()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
