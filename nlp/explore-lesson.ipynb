{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP EDA\n",
    "\n",
    "Text Data Exploration Techniques\n",
    "\n",
    "- [Term Frequency](#Term-Frequency)\n",
    "- [Ngrams](#Ngrams)\n",
    "- [Document Length](#Document-Length)\n",
    "- [Word Cloud](#Word-Cloud)\n",
    "- [Sentiment Analysis](#Sentiment-Analysis)\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# matplotlib default plotting styles\n",
    "plt.rc(\"patch\", edgecolor=\"black\", force_edgecolor=True)\n",
    "plt.rc(\"axes\", grid=True)\n",
    "plt.rc(\"grid\", linestyle=\":\", linewidth=0.8, alpha=0.7)\n",
    "plt.rc(\"axes.spines\", right=False, top=False)\n",
    "plt.rc(\"figure\", figsize=(11, 8))\n",
    "plt.rc(\"font\", size=12.0)\n",
    "plt.rc(\"hist\", bins=25)\n",
    "\n",
    "def clean(text: str) -> List[str]:\n",
    "    \"a simple function to prepare text data\"\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\") + [\"r\", \"u\", \"2\", \"ltgt\"]\n",
    "    text = (\n",
    "        unicodedata.normalize(\"NFKD\", text)\n",
    "        .encode(\"ascii\", \"ignore\")\n",
    "        .decode(\"utf-8\", \"ignore\")\n",
    "        .lower()\n",
    "    )\n",
    "    words = re.sub(r\"[^\\w\\s]\", \"\", text).split()\n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]\n",
    "\n",
    "df = pd.read_csv(\"spam_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What percentage of the data is spam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.value_counts().plot.pie(\n",
    "    colors=[\"pink\", \"lightblue\"], explode=(0.15, 0), autopct=\"%.0f%%\"\n",
    ")\n",
    "plt.title(\"Ham vs Spam Distribution\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(\"n = %d\" % df.shape[0])\n",
    "\n",
    "pd.concat(\n",
    "    [df.label.value_counts(), df.label.value_counts(normalize=True)], axis=1\n",
    ").set_axis([\"n\", \"percent\"], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\" \".join(df.text).split()).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. one big string for everything, spam, ham\n",
    "1. lists of strings\n",
    "1. list of strings -> pandas series so we can value count\n",
    "1. combine series into single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \" \".join(df.text)\n",
    "spam_text = \" \".join(df[df.label == \"spam\"].text)\n",
    "ham_text = \" \".join(df[df.label == \"ham\"].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = clean(all_text)\n",
    "spam_words = clean(spam_text)\n",
    "ham_words = clean(ham_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_freq = pd.Series(all_words).value_counts()\n",
    "spam_freq = pd.Series(spam_words).value_counts()\n",
    "ham_freq = pd.Series(ham_words).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = (\n",
    "    pd.concat([all_freq, ham_freq, spam_freq], axis=1, sort=True)\n",
    "    .rename(columns={0: \"all\", 1: \"ham\", 2: \"spam\"})\n",
    "    .fillna(0)\n",
    "    .apply(lambda col: col.astype(int))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- most common words overall?\n",
    "- most common spam, ham words?\n",
    "- any words that uniquely spam or ham?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.sort_values(by=\"all\").tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngrams\n",
    "\n",
    "- bigrams + viz most frequent for all, spam, ham\n",
    "- trigrams, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nltk.bigrams('I love the smell of regex in the morning'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(nltk.bigrams(all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"message_length\"] = df.text.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, y=\"message_length\", x=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(\"message_length\", by=\"label\", sharex=True, layout=(2, 1), bins=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"n_words\"] = df.text.str.count(r\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"label\").n_words.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_axes([0.1, 0.1, 0.4, 0.8])  # left, bottom, width, height\n",
    "ax2 = fig.add_axes([0.55, 0.1, 0.4, 0.35])\n",
    "ax3 = fig.add_axes([0.55, 0.55, 0.4, 0.35], sharex=ax2)\n",
    "sns.boxplot(data=df, y=\"n_words\", x=\"label\", ax=ax1)\n",
    "df.hist(\"n_words\", by=\"label\", bins=25, ax=[ax2, ax3])\n",
    "fig.suptitle(\"Distribution of Number of Words for Spam and Ham Messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python -m pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`WordCloud()` produces an image object, which can be displayed with `plt.imshow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "sentence = (\n",
    "    \"Mary had a little lamb, little lamb, little lamb. Its fleece was white as snow.\"\n",
    ")\n",
    "img = WordCloud(background_color=\"white\").generate(sentence)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do the same with all words, spam and ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = WordCloud(background_color=\"white\").generate(all_text)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cloud = WordCloud(background_color=\"white\", height=1000, width=400).generate(\n",
    "    \" \".join(all_words)\n",
    ")\n",
    "ham_cloud = WordCloud(background_color=\"white\", height=600, width=800).generate(\n",
    "    \" \".join(ham_words)\n",
    ")\n",
    "spam_cloud = WordCloud(background_color=\"white\", height=600, width=800).generate(\n",
    "    \" \".join(spam_words)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "axs = [\n",
    "    plt.axes([0, 0, 0.5, 1]),\n",
    "    plt.axes([0.5, 0.5, 0.5, 0.5]),\n",
    "    plt.axes([0.5, 0, 0.5, 0.5]),\n",
    "]\n",
    "\n",
    "axs[0].imshow(all_cloud)\n",
    "axs[1].imshow(ham_cloud)\n",
    "axs[2].imshow(spam_cloud)\n",
    "\n",
    "axs[0].set_title(\"All Words\")\n",
    "axs[1].set_title(\"Ham\")\n",
    "axs[2].set_title(\"Spam\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud with Bigrams\n",
    "\n",
    "- `generate_from_frequencies` + python gymnastics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = {\n",
    "    \"Codeup\": 10,\n",
    "    \"Bayes\": 5,\n",
    "    \"Data Science\": 6,\n",
    "}\n",
    "\n",
    "img = WordCloud(background_color=\"white\").generate_from_frequencies(frequencies)\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_ham_bigrams = pd.Series(nltk.bigrams(ham_words)).value_counts().head(20)\n",
    "\n",
    "data = {p1 + \" \" + p2: v for (p1, p2), v in top_20_ham_bigrams.to_dict().items()}\n",
    "\n",
    "img = WordCloud(\n",
    "    background_color=\"white\", width=800, height=400\n",
    ").generate_from_frequencies(data)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "A way for us to put a number to indicate whether a document has a positive or\n",
    "negative sentiment.\n",
    "\n",
    "### Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"Sentiment analysis is very awesome!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"I am pretty worried about bad weather this weekend.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"vader_sentiment\"] = df.text.apply(lambda txt: sia.polarity_scores(txt)[\"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, y=\"vader_sentiment\", x=\"label\")\n",
    "df.groupby(\"label\").vader_sentiment.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Afinn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python -m pip install Afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "\n",
    "sa = Afinn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa.score(\"Sentiment analysis is very awesome!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa.score(\"I am pretty worried about bad weather this weekend.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"afinn_sentiment\"] = df.text.apply(sa.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, y=\"afinn_sentiment\", x=\"label\")\n",
    "df.groupby(\"label\").afinn_sentiment.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- [VADER Sentiment Analysis](https://github.com/cjhutto/vaderSentiment)\n",
    "- [AFINN Sentiment Analysis](https://github.com/fnielsen/afinn)\n",
    "\n",
    "## Other NLP Libraries\n",
    "\n",
    "- [spaCy](https://spacy.io/)\n",
    "- [textacy](https://chartbeat-labs.github.io/textacy/) builds on top of spaCy\n",
    "- [TextBlob](https://textblob.readthedocs.io/en/dev/)\n",
    "\n",
    "## Bonus Exercises\n",
    "\n",
    "After you've worked through the exercises in the curriculum,\n",
    "\n",
    "- Use sentiment analysis to explore your datasets. Which news category has the highest sentiment? Which has the lowest? Does this match with what you might predict?\n",
    "\n",
    "- Create a feature named `has_long_words`. This should be either true or false depending on whether or not the message contains a word greater than 5 characters. Use this feature to explore spam v ham. What changes if you change the cutoff from 5 to 8 characters?\n",
    "\n",
    "- Explore the enron spam email dataset\n",
    "\n",
    "    Download [the json file located here](https://ds.codeup.com/enron_spam.json.gz) and read it with pandas:\n",
    "    \n",
    "    We have done a little preprocessing and acquisition of the [data found in this kaggle competition](https://www.kaggle.com/wanderfj/enron-spam) to make it easier to work with.\n",
    "    \n",
    "    ```python\n",
    "    df = pd.read_json(\"enron_spam.json.gz\")\n",
    "    ```\n",
    "    \n",
    "    Start by focusing just on the `label`, `subject`, and `text` columns."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
