{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do your work for these exercises in either a notebook or file name `model`\n",
    "from acquire import get_iris_data\n",
    "from acquire import get_titanic_data\n",
    "from prepare import prep_titanic\n",
    "from prepare import prep_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) Fit the logistic regression classifier to your training sample and transform,i.e. make predictions\n",
    "#on the training sample\n",
    "iris_df=prep_iris(get_iris_data())\n",
    "y=iris_df[['species']]\n",
    "X=iris_df[['petal_length','petal_width','sepal_length','sepal_width']]\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,train_size=.7,random_state=123)\n",
    "lr=LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "pred_y=lr.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2)Evaluate your in-sample results using the model score,confusion matrix and classification report.\n",
    "print(lr.score(X_train,y_train))\n",
    "print(confusion_matrix(y_train,pred_y))\n",
    "labels=sorted(iris_df.species_name.unique())\n",
    "print(pd.DataFrame(confusion_matrix(y_train,pred_y),index=labels,columns=labels))\n",
    "print(classification_report(y_train,pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) Print and clearly label the following:Accuracy,true positive rate,false positive rate,true negative rate,false negative rate,\n",
    "# false negative rate,precision,recall,f1-score,and support. \n",
    "confusion_matrix1=pd.DataFrame(confusion_matrix(y_train,pred_y),index=labels,columns=labels)\n",
    "FP = confusion_matrix1.sum(axis=0) - np.diag(confusion_matrix1)  \n",
    "FN = confusion_matrix1.sum(axis=1) - np.diag(confusion_matrix1)\n",
    "TP = np.diag(confusion_matrix1)\n",
    "TN = confusion_matrix1.values.sum() - (FP + FN + TP)\n",
    "\n",
    "#True positive rate/recall\n",
    "TPR = TP/(TP+FN)\n",
    "# True negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# False positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "# F1-score\n",
    "f1_score=2*TP/(2*TP+FP+FN)\n",
    "\n",
    "#support\n",
    "support=len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4) Look in the scikit-learn documentation to research the solver parameter. What is your best option\n",
    "#for the particular problem you are trying to solve and the data to be used.\n",
    "\n",
    "\n",
    "\n",
    "#5) Run through steps 2-4 using another solver\n",
    "logit_fit=LogisticRegression(solver='saga')\n",
    "logit_fit.fit(X_train,pd.DataFrame(y_train))\n",
    "pred_logit=logit_fit.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6) Which performs better on your in-sample data?\n",
    "print(pred_logit.score(X_train,pd.DataFrame(y_train)))\n",
    "print(pd.DataFrame(confusion_matrix(y_train,pred_logit),index=labels,columns=labels))\n",
    "print(classification_report(y_train,pred_logit))\n",
    "#7)Save the best model in logit_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "\n",
    "#1)Fit the decision tree classifier to your training sample and transform(i.e. make predictions on your training sample)\n",
    "dt=DecisionTreeClassifier()\n",
    "dt.fit(X_train,y_train)\n",
    "pred_tree=dt.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2)Evaluate your in-sample results using the model score,confusion matrix,and classification report.\n",
    "print(dt.score(X_train,y_train))\n",
    "print(pd.DataFrame(confusion_matrix(y_train,pred_tree),index=labels,columns=labels))\n",
    "print(classification_report(y_train,pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-550919799e62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#3) Print and clearly label the following:Accuracy,true positive rate,false positive rate,true negative rate,false negative rate,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# false negative rate,precision,recall,f1-score,and support.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mconfusion_matrix1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_tree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mFP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mFN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#3) Print and clearly label the following:Accuracy,true positive rate,false positive rate,true negative rate,false negative rate,\n",
    "# false negative rate,precision,recall,f1-score,and support.\n",
    "confusion_matrix1=pd.DataFrame(confusion_matrix(pred_tree,pred_y),index=labels,columns=labels)\n",
    "FP = confusion_matrix1.sum(axis=0) - np.diag(confusion_matrix1)  \n",
    "FN = confusion_matrix1.sum(axis=1) - np.diag(confusion_matrix1)\n",
    "TP = np.diag(confusion_matrix1)\n",
    "TN = confusion_matrix1.values.sum() - (FP + FN + TP)\n",
    "\n",
    "#True positive rate/recall\n",
    "TPR = TP/(TP+FN)\n",
    "# True negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# False positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "# F1-score\n",
    "f1_score=2*TP/(2*TP+FP+FN)\n",
    "\n",
    "#support\n",
    "support=len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4)Run through 2-4 using entropy as your measure of impurity\n",
    "tree_fit=DecisionTreeClassifier(criterion='entropy')\n",
    "tree_fit.fit(X_train,y_train)\n",
    "pred_tree=tree_fit.predict(X_train)\n",
    "print(tree_fit.score(X_train,y_train))\n",
    "print(pd.DataFrame(confusion_matrix(y_train,pred_tree),index=labels,columns=labels))\n",
    "print(classification_report(y_train,pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#5) Which performs better on your in-sample data?\n",
    "# They both performed the same\n",
    "\n",
    "#6)Save the best model in `tree_fit`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
