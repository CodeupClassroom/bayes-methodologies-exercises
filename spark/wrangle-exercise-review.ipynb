{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "from pyspark.sql.functions import (\n",
    "    expr,\n",
    "    col,\n",
    "    to_timestamp,\n",
    "    format_string,\n",
    "    regexp_extract,\n",
    "    datediff,\n",
    "    current_timestamp,\n",
    "    when,\n",
    "    max,\n",
    "    lit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_311_data(spark: pyspark.sql.SparkSession) -> pyspark.sql.DataFrame:\n",
    "    print(\"[wrangle.py] reading case.csv\")\n",
    "    df = spark.read.csv(\"data/case.csv\", header=True, inferSchema=True)\n",
    "    return df.withColumnRenamed(\"SLA_due_date\", \"case_due_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_dtypes(df: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame:\n",
    "    print(\"[wrangle.py] handling data types\")\n",
    "    return (\n",
    "        df.withColumn(\"case_closed\", expr('case_closed == \"YES\"'))\n",
    "        .withColumn(\"case_late\", expr('case_late == \"YES\"'))\n",
    "        .withColumn(\"council_district\", col(\"council_district\").cast(\"string\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_dates(df: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame:\n",
    "    print(\"[wrangle.py] parsing dates\")\n",
    "    fmt = \"M/d/yy H:mm\"\n",
    "    return (\n",
    "        df.withColumn(\"case_opened_date\", to_timestamp(\"case_opened_date\", fmt))\n",
    "        .withColumn(\"case_closed_date\", to_timestamp(\"case_closed_date\", fmt))\n",
    "        .withColumn(\"case_due_date\", to_timestamp(\"case_due_date\", fmt))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame:\n",
    "    print(\"[wrangle.py] adding features\")\n",
    "    max_date = df.select(max(\"case_closed_date\")).first()[0]\n",
    "    return (\n",
    "        df.withColumn(\"num_weeks_late\", expr(\"num_days_late / 7 AS num_weeks_late\"))\n",
    "        .withColumn(\n",
    "            \"council_district\",\n",
    "            format_string(\"%03d\", col(\"council_district\").cast(\"int\")),\n",
    "        )\n",
    "        .withColumn(\"zipcode\", regexp_extract(\"request_address\", r\"\\d+$\", 0))\n",
    "        .withColumn(\"case_age\", datediff(lit(max_date), \"case_opened_date\"))\n",
    "        .withColumn(\"days_to_closed\", datediff(\"case_closed_date\", \"case_opened_date\"))\n",
    "        .withColumn(\n",
    "            \"case_lifetime\",\n",
    "            when(expr(\"! case_closed\"), col(\"case_age\")).otherwise(\n",
    "                col(\"days_to_closed\")\n",
    "            ),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_departments(\n",
    "    case_df: pyspark.sql.DataFrame, spark: pyspark.sql.SparkSession\n",
    ") -> pyspark.sql.DataFrame:\n",
    "    print(\"[wrangle.py] joining departments\")\n",
    "    dept = spark.read.csv(\"data/dept.csv\", header=True, inferSchema=True)\n",
    "    return (\n",
    "        case_df.join(dept, \"dept_division\", \"left\")\n",
    "        # drop all the columns except for standardized name, as it has much fewer unique values\n",
    "        .drop(dept.dept_division)\n",
    "        .drop(dept.dept_name)\n",
    "        .drop(case_df.dept_division)\n",
    "        .withColumnRenamed(\"standardized_dept_name\", \"department\")\n",
    "        # convert to a boolean\n",
    "        .withColumn(\"dept_subject_to_SLA\", col(\"dept_subject_to_SLA\") == \"YES\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_311(spark: pyspark.sql.SparkSession) -> pyspark.sql.DataFrame:\n",
    "    df = add_features(handle_dates(handle_dtypes(get_311_data(spark))))\n",
    "    return join_departments(df, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wrangle.py] reading case.csv\n",
      "[wrangle.py] handling data types\n",
      "[wrangle.py] parsing dates\n",
      "[wrangle.py] adding features\n",
      "[wrangle.py] joining departments\n"
     ]
    }
   ],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "df = wrangle_311(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|zipcode|\n",
      "+-------+\n",
      "|  78207|\n",
      "|  78223|\n",
      "|  78223|\n",
      "|  78223|\n",
      "|  78228|\n",
      "|       |\n",
      "|  78251|\n",
      "|  78251|\n",
      "|  78251|\n",
      "|  78251|\n",
      "|  78251|\n",
      "|  78251|\n",
      "|  78251|\n",
      "|  78251|\n",
      "|  78251|\n",
      "|  78251|\n",
      "|  78251|\n",
      "|  78251|\n",
      "|  78251|\n",
      "|  78251|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('zipcode').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(service_request_type='Stray Animal', department='Animal Care Services')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = df.select('service_request_type', 'department').head()\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stray Animal'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.service_request_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 8, 8, 10, 38)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = df.select(max('case_closed_date')).head()\n",
    "row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|case_lifetime|\n",
      "+-------------+\n",
      "|            0|\n",
      "|            2|\n",
      "|            1|\n",
      "|            1|\n",
      "|            0|\n",
      "|            0|\n",
      "|            1|\n",
      "|            1|\n",
      "|            1|\n",
      "|            1|\n",
      "|            1|\n",
      "|            1|\n",
      "|            1|\n",
      "|            1|\n",
      "|            1|\n",
      "|            1|\n",
      "|            1|\n",
      "|            1|\n",
      "|            1|\n",
      "|            1|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('case_lifetime').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
